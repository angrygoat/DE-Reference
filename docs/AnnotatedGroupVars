# Annotated group_var reference for Discovery Environment.
# Required but sanitized variables are valued CHANGEME.
# Note that in general variable names are converting
# from long_strings to service.stanzas.

# We run our playbooks with sudo (-s -K) and so need to specify a user here.  This is the user name under which
# the ansible playbooks are run on the de boxes
ansible_ssh_user: CHANGEME

# Master iPlant config locations
# Note: The webapp currently requires that the de.properties file be located in /etc/iplantc/de/.

# this is the etc location where de related configs will be placed.  Don't change this.
global_config_dir: /etc/iplant
+
# subfolder under /etc/iplant for configs for de, don't change this either
de_config_dir: "{{ global_config_dir }}/de"
# This is required by generate-local-configs.yaml.
local_cfg_dest: CHANGEME

java:
  version: 1.7.0

# Indicates whether or not this environment is a complete environment
# or piggybacks off some other environment
parasitic: false

max_heap:
  low: "512M"
  high: "1G"

################################################################################
#                         Configuration Settings
################################################################################

# this is the cas entitlement value that indicates a user can be an admin, and log into belphegor, the admin web interface
# TODO: check on status of patch compared to odum, which is using an LDAP DN
admin_groups: de_admin

# internal listening port for dockerized services, suggested that this does not change
default_service_port: 60000

# Non-iPlant organizations will need to obtain client keys to use Agave.
# Even if you're not connected, you can still use the public bits.
# Agave may be disabled entirely by switching flags in the apps.properties template.
agave:
  # this turns off agave services in the DE and other agave settings below won't do things
  apps_features: false
  # this should be turned off if apps_features is turned off, don't use agave if this is set to false
  apps_features_jobs: false
  base_url: https://agave.iplantc.org
  read_timeout: 10000
  page_length: 5000
  oauth_refresh_window: 5
  client_key: CHANGEME
  client_secret: CHANGEME

# AMQP for event notifications, note that the 'flat' variables still seem to be used in some playbooks, but
# they are referenced from the structured variables to minimize confusion, leave them there for now
amqp_broker:
  host: "{{ groups['amqp-brokers'][0] }}"
  port: 5672
  mgmt_port: 15672
  password: "{{amqp_password}}"
  user: "{{amqp_user}}"
  condor_events:
    exchange: condor_events
    exchange_type: fanout
    exchange_durable: true
    exchange_routing_key: CHANGEME
    queue_name: CHANGEME
    exchange_auto_delete: false
  de:
    exchange: "{{ amqp_de_exchange }}"
    exchange_durable: "{{ amqp_de_exchange_durable}}"
    exchange_auto_delete: "{{ amqp_de_exchange_auto_delete}}"
  irods:
    connection_health_check_interval: "{{amqp_irods_connection_health_check_interval}}"
    exchange: "{{amqp_irods_exchange}}"
    exchange_type: "{{amqp_irods_exchange_type}}"
    exchange_durable: "{{amqp_irods_exchange_durable}}"
    exchange_auto_delete: "{{amqp_irods_exchange_auto_delete}}"
    message_auto_ack: true
    queue_routing_key: "data-object.#"

amqp_password: CHANGEME
amqp_user: CHANGEME
amqp_de_exchange: de
amqp_de_exchange_durable: true
amqp_de_exchange_auto_delete: false
amqp_irods_exchange: irods
amqp_irods_exchange_type: topic
amqp_irods_exchange_durable: true
amqp_irods_exchange_auto_delete: false
amqp_irods_connection_health_check_interval: 5000

# TODO: This flattened ref to inventory extracted vars was done for ansible bugs. It didn't like these inventory
# refs embedded in the nested variables
anon_port: 60000
anon_host:  "{{ groups['anon-files'][0] }}"

anon_files:
 host: "{{ anon_host }}"
 port: "{{ anon_port }}"
 base: "http://{{anon_host}}:{{anon_port}}"
 proxy_url: "https://{{ groups['ui'][0] }}/anon-files/"
 anon_user: anonymous
 service_name: anon-files.service
 service_name_short: anon-files
 compose_service: anon_files
 service_description: anon-files service
 image_name: anon-files
 log_driver: "{{ docker.log_driver }}"
 container_name: anon-files
 properties_file: anon-files.properties
 log_file: anon-files-docker.log
 max_heap: "{{ max_heap.low }}"

compose:
  # don't change this - hard-coded in admin(?) interface
  de_env: de
  de_tag: latest

## App Settings
de:
  host: "{{ groups['ui'][0] }}"
  base: "https://{{ groups['ui'][0] }}/de"
  app_base: "https://{{ groups['ui'][0] }}"
  service_name: de-ui.service
  service_name_short: ui
  service_description: DE UI; iPlant Discovery Environment user interface
  image_name: de-ui
  compose_service: de_ui
  container_name: de-ui
  docker_repository: discoenv # update to cause de-ui to be pulled from an alternative location
  log_driver: "{{ docker.log_driver }}"
  log_file: de-ui.log
  context_menu_enabled: false
  description: the CyVerse Discovery Environment
  empty_url: empty
  app_name: de
  rpc_name: discoveryenvironment
  notification_poll: 15
  maintenance_file: de-maintenance
  http_server:
    service_name: de-ui-nginx.service
    service_name_short: de-ui-nginx
    service_description: DE UI nginx
    image_name: nginx-ssl
    container_name: de_ui_nginx
    compose_service: de_ui_nginx
    log_driver: "{{ docker.log_driver }}"
    log_file: nginx-de-ui.log
    ssl:
      server_name: "{{ nginx_ssl.server_name }}"
      cert: "{{ nginx_ssl.cert }}"
      cert_key: "{{ nginx_ssl.cert_key }}"
      insecure_redirects:
        - server_name: "{{ nginx_ssl.server_name }}"
          return: "https://$host$request_uri"
      tomcat_ks_cert: CHANGEME
      tomcat_ks_chain: CHANGEME
      tomcat_ks_pass: CHANGEME
#      redirects:
#        - server_name: "~^(?<basename>[^.]+)[.]example[.]com$"
#          return: "https://$basename.example.org$request_uri"
#          ssl_certificate: "/etc/ssl/example.com.crt"
#          ssl_certificate_key: "/etc/ssl/example.com.key"

# populates the "About" dialog, shouldn't affect functionality
app_version_name: Phthalo
app_version: 2.4.0

apps_host: "{{ groups['apps'][0] }}"
apps_port: 5007

apps:
 host: "{{apps_host }}"
 port: "{{apps_port}}"
 base: "http://{{apps_host }}:{{apps_port}}"
 service_name: apps.service
 service_name_short: apps
 compose_service: apps
 service_description: apps service
 image_name: apps
 container_name: apps
 properties_file: apps.properties
 log_file: apps-docker.log
 out_dir: analyses
 path_list_max_paths: 16
 path_list_max_size: 1048576
 beta_category: 5401bd14-6c14-4470-aedd-57b47ea1b979
 user_root: Workspace
 user_subs: "[\"Apps under development\",\"Favorite Apps\"]"
 trash_category: Trash
 max_heap: "{{ max_heap.high }}"

app_server_base_url: https://{{ app_server_hostname }}
app_server_hostname: "{{ groups['ui'][0] }}"

de_feedback_to_addr: "CHANGEME"
de_mail_from_addr: "{{ de_feedback_to_addr }}"
de_mail_to_addr: "{{ de_feedback_to_addr }}"


# CAS Authentication Settings are used by de-properties
org.iplantc.discoveryenvironment.cas.base-url: "{{ cas.base }}"
org.iplantc.discoveryenvironment.cas.server-name: "https://{{ cas.host}}:{{ cas.port }}"
org.iplantc.discoveryenvironment.cas.validation: /iplant-cas-ticket-validator
org.iplantc.discoveryenvironment.cas.logout-url: /iplant-cas-logout
org.iplantc.discoveryenvironment.cas.app-name: DFC Discovery Environment
org.iplantc.discoveryenvironment.cas.login-url: /login
org.iplantc.admin.cas.authorized-groups: "{{ admin_groups }}"
org.iplantc.admin.cas.group-attribute-name: entitlement
org.iplantc.discoveryenvironment.cas.no-logout-url: "https://{{ cas.base }}"
org.iplantc.discoveryenvironment.cas.app-list: all iPlant applications
org.iplantc.discoveryenvironment.keepalive.service: "https://{{ cas.host }}:{{ cas.port }}/de/discoveryenvironment/empty"
org.iplantc.discoveryenvironment.keepalive.target: "https://{{ cas.host }}:{{ cas.port }}/cas/login?service=https://{{ nginx_ssl.server_name }}/de/discoveryenvironment/empty"
org.iplantc.discoveryenvironment.keepalive.interval: 90

# location of cas server, usually the ui box, change here if different
cas_base: "https://{{ groups['ui'][0] }}"

cas:
  app_list: all DFC applications
  base: "{{cas_base}}"
  context_path: cas
  do_ssl_config: true
  git_url: https://changeme
  git_project_name: cas-overlay
  # this is the attribute from cas that will contain the groups that a user is assigned to
  group_attribute: entitlement
  no_logout_url: http://some.server1.at.a.place
  port: 8443
  uid_domain: example.org
  # directory on the configured cas server that will be created and will receive the ssl keypair for tomcat, define
  # without trailing slash
  ssl_certificate_server_dir:
  # directory on the ansible head node that will contain the source keypair, define without trailing slash
  ssl_certificate_source_dir:
  # file name of public key on ansible head node to copy to cas server (not the full path, this is under the ssl_certificate_source_dir)
  ssl_cert_file:
  # file name of private key on ansible head node to copy to cas server (not the full path, this is under the ssl_certificate_source_dir)
  ssl_key_file:
  # tomcat admin user name and password
  tomcat_admin_username: admin
  tomcat_admin_password: CHANGEME
  # system user/group running tomcat
  tomcat_user: tomcat
  tomcat_group: tomcat


# we don't use this,
chat_room_url: CHANGEME

# drop_number is used by iPlant for QA. others may safely ignore it.
drop_number: 0

# The clockwork service is no longer used.
# TODO: is this even used?  Don checking with Dennis

clockwork:
  service_name: clockwork.service
  service_name_short: clockwork
  service_description: clockwork service
  image_name: clockwork
  container_name: clockwork
  compose_service: clockwork
  properties_file: clockwork.properties
  log_file: clockwork-docker.log
  max_heap: "{{ max_heap.low }}"

# Condor handles all DE job execution; Jex performs its care and feeding.

# this is a condor group quota setting and should be numeric
group_config: 0
condor_submission_ip_range: CHANGEME
condor_allow_write: "{{ condor_submission_ip_range }}"
condor:
  host: "{{ groups['condor-submission'][0] }}"
  admin: CHANGEME
  collector_name: "{{ environment_name }} pool"
  cred_dir: /var/cred_dir
  flock_to: CHANGEME
  filesystem_domain: "{{ ansible_fqdn }}"
  rhel7_repo: htcondor-stable-rhel7.repo
  uid_domain: CHANGEME
  allow_write: "{{ condor_allow_write }}"
  allow_read: "{{ condor_allow_write }}"

# Condor Log Monitor does exactly that, and reports to jexevents.
condor_log_monitor:
  event_log: /var/log/condor/event_log
  service_name: condor-log-monitor.service
  service_name_short: condor-log-monitor
  service_description: CLM; Condor log monitor
  image_name: condor-log-monitor
  container_name: clm
  compose_service: clm
  properties_file: condor_log_monitor.properties
  log_file: condor-log-monitor-docker.logt5g

# Coge is unavailable to external entities.
coge_genome_load_url: https://genomevolution.org/CoGe/services/service.pl/genome/load
coge_base_url: https://genomevolution.org/coge/api/v1
coge_data_folder_name: coge_data
coge_user: coge

coge:
  user: coge
  base_url: https://genomevolution.org/coge/api/v1
  data_folder_name: coge_data


# iPlant data container settings
data_container:
  image_name: CHANGEME
  container_name: de-data
  service_name: iplant-data.service
  service_name_short: iplant-data
  compose_service: iplant_data
  service_description: The Discovery Environment data container
  ssl:
    cert: "{{ nginx_ssl.cert }}"
    key: "{{ nginx_ssl.cert_key }}"
    gd_bundle_crt: /etc/ssl/gd_bundle.crt
  keystore:
    path: /etc/ssl/example.pkcs12
    password:
    type: pkcs12


# data-info is a RESTful frontend for getting information about
# and manipulating information in an iRODS data store.
data_info_host: "{{ services_host }}"
data_info:
  host: "{{ groups['data-info'][0] }}"
  port: 5001
  service_name: data-info.service
  service_name_short: data-info
  service_description: data-info service
  compose_service: data_info
  image_name: data-info
  container_name: data-info
  properties_file: data-info.properties
  log_file: data-info-docker.log
  max_heap: "{{ max_heap.high }}"

# version info for de
de_version: "{{ app_version }}"
de_version_name: "{{ app_version_name }}"

db_driver: org.postgresql.Driver
db_user: CHANGEME
db_password: CHANGEME
db_host: "{{ groups['db'][0] }}"
db_name: CHANGEME
db_port: 5432
db_admin: postgres
db_admin_password: CHANGEME
db_targz: https://everdene.iplantcollaborative.org/jenkins/job/databases-dev/lastSuccessfulBuild/artifact/databases/de-database-schema/database.tar.gz

db_vendor: postgresql

# Incremental iRODS indexer.
dewey:
  service_name: dewey.service
  service_name_short: dewey
  listen_port: 5002
  compose_service: dewey
  service_description: dewey service
  image_name: dewey
  container_name: dewey
  properties_file: dewey.properties
  log_file: dewey-docker.log
  max_heap: "{{ max_heap.low }}"


# Docker registry settings. You'll want your own docker registry
# to host private data containers.
docker:
  # eth0 setting used by iptables role for docker port forwarding.
  eth0: eno16777984
  log_driver: syslog
  tag: latest
  user: discoenv
  version: 1.11.0
  compose_path: /etc/compose.yaml
  registry:
    host: "{{ groups['docker-registry'][0] }}"
    port: 443
    base: "CHANGEME"
    user: CHANGEME
    pass: CHANGEME
    login: yes
    # directory that will be the source of the public key on the ansible head node that will be copied to the
    # docker enabled ansible nodes, generate keys using openssh and place here on the box where you run ansible
    cert_dir: /home/ansible/de/ansible-vars/localdata/dockercerts # no trailing slash
    target_config_dir: /etc/iplant/de/docker-registry/certs/dockercert/ # trailing slash needed
    # public/private keys for private docker registry as set up  by the docker_repo role
    public_key: server.crt
    private_key: server.key
  internal_registry: CHANGEME # name of internal docker repo e.g. my.example.com:443 where you run your private docker repo

email_smtp_host: CHANGEME
email_smtp_from_address: CHANGEME
email_host: CHANGEME
email_base: http://{{ email_host }}:{{ iplant_email.port }}

# don't change this - some things are still hard-coded.
environment_name: de

# for ansible-elasticsearch role https://github.com/cyverse/ansible-elasticsearch
elasticsearch_user: elasticsearch
elasticsearch_group: elasticsearch
elasticsearch_heap_size: 2g
elasticsearch_cluster_name: CHANGEME

elasticsearch_network_http_port: 9200
elasticsearch:
  host: "{{ groups['elasticsearch'][0] }}"
  port: 9200
  base: "http://{{ groups['elasticsearch'][0] }}:9200"
  scroll_size: 1000
  cluster_name: elasticsearch
  heap_size:
  network_http_port:
  network_transport_tcp_port:

elk_host: "{{ groups['elk'][0] }}"
elk:
  conf_dir: "{{de_config_dir}}/elk"
  logstash:
    port: 5000
    container_name: elk_logstash
    service_name: elk-logstash.service
    service_name_short: elk-logstash
    service_description: ELK logstash
    image_name: de-logstash
  data:
    container_name: elk_data
    service_name: elk-data.service
    service_name_short: elk-data
    service_description: ELK data container
    image_name: busybox
  elasticsearch:
    port: 9200
    container_name: elk_elasticsearch
    service_name: elk-elasticsearch.service
    service_name_short: elk-elasticsearch
    service_description: ELK elasticsearch
    image_name: elasticsearch
    heap_size: "12g"
    cluster_name: "de-elk-dev"
  kibana:
    port: 5601
    container_name: elk_kibana
    service_name: elk-kibana.service
    service_name_short: elk-kibana
    service_description: ELK kibana
    version: 4.2
    image_name: "kibana:4.2"

exim:
  service_name: exim-sender.service
  service_name_short: exim-sender
  compose_service: exim_sender
  service_description: exim-sender service
  image_name: exim-sender
  container_name: exim
  log_file: exim-docker.log

# http proxy settings for facepalm. you may leave these blank.
facepalm_proxy_host:
facepalm_proxy_port:

fs_max_paths_in_request: 1000

gpg_home_dir:

grouper:
  config_dir: /etc/grouper
  service_name: iplant-grouper.service
  service_name_short: iplant-grouper
  compose_service: iplant_grouper
  service_description: Grouper UI and Web Services
  image_name: grouper
  image_tag: 2.2.2
  init: false
  init_image: sharkbait
  container_name: iplant-grouper
  log_driver: "{{ docker.log_driver }}"
  max_heap_size: 2048M
  max_perm_size: 256M
  ssl_certificate: CHANGEME
  ssl_certificate_key: CHANGEME
  admin:
    user: GrouperSystem
    pass: CHANGEME
  api:
    env_name: de
    container_name: grouper
  db:
    url: "jdbc:postgresql://{{db_host}}/subjectdata"
    user: degrouper
    pass: CHANGEME
  http_server:
    service_name: grouper-nginx.service
    service_name_short: grouper-nginx
    service_description: Grouper nginx
    image_name: "{{ de.http_server.image_name }}"
    container_name: grouper-nginx
    log_driver: "{{ docker.log_driver }}"
    ssl:
      servers:
        - server_name: "{{ groups['grouper'][0] }}"
          #ssl_certificate: "{{ nginx_ssl.cert }}"
          ssl_certificate: /etc/iplant/ssl/de-grouper_irss_unc_edu_cert.cer
          #ssl_certificate_key: "{{ nginx_ssl.cert_key }}"
          ssl_certificate_key: /etc/iplant/ssl/de-grouper_irss_unc_edu.key
      insecure_redirects:
        - server_name: "{{ nginx_ssl.server_name }}"
          return: "https://$host$request_uri"
  subject_source:
    id: DE
    name: DE
    url: ldap://"{{ groups['ldap'][0] }}:{{ ldap.port }}"
    auth_type: simple
    principal: "{{ ldap.manager_dn }}"
    credentials: "{{ ldap.admin_password }}"
  ui:
    base_url: "https://{{ groups['ui'][0] }}"
  ws:
    base_url: "http://{{ groups['grouper'][0] }}:8080/grouper-ws/"

# Periodic indexer for iRODS.
infosquito:
  host: "{{ groups['infosquito'][0] }}"
  service_name: infosquito.service
  service_name_short: infosquito
  compose_service: infosquito
  service_description: infosquito service
  image_name: infosquito
  container_name: infosquito
  properties_file: infosquito.properties
  log_file: infosquito-docker.log
  notify_enabled: true
  notify_count: 10000
  retry_interval: 900
  max_heap: "{{ max_heap.low }}"

# Facilitates file extension parsing in the GUI.
info_typer:
  host: "{{ groups['info-typer'][0] }}"
  service_name: info-typer.service
  service_name_short: info-typer
  service_description: info-typer service
  image_name: info-typer
  compose_service: info_typer
  container_name: info-typer
  properties_file: info-typer.properties
  log_file: info-typer-docker.log
  max_heap: "{{ max_heap.low }}"


iplant_email_host:  "{{ groups['iplant-email'][0] }}"
iplant_email_port: 587

email_smtp_host: 127.0.0.1
email_smtp_from_address: changeme@example.org
iplant_email:
 host: "{{iplant_email_host}}"
 port: "{{iplant_email_port}}"
 base: "http://{{ iplant_email_host }}:{{iplant_email_port}}"
 service_name: iplant-email.service
 service_name_short: iplant-email
 compose_service: iplant_email
 service_description: iPlant Email service
 image_name: iplant-email
 container_name: iplant-email
 properties_file: iplant-email.properties
 log_file: iplant-email-docker.log
 max_heap: "{{ max_heap.low }}"


# A RESTful façade in front of Grouper.
iplant_groups:
  host: "{{ groups['iplant-groups'][0] }}"
  port: 5012
  base_url: "http://{{ groups['iplant-groups'][0] }}:5012"
  #base_url: "http://{{ iplant_groups_host }}:{{ iplant_groups_port }}"
  service_name: iplant-groups.service
  service_name_short: iplant-groups
  service_description: iplant-groups service
  image_name: iplant-groups
  container_name: iplant-groups
  compose_service: iplant_groups
  properties_file: iplant-groups.properties
  log_file: iplant-groups-docker.log
  grouper:
    username: "{{ grouper.admin.user }}"
    password: "{{ grouper.admin.pass }}"
    api_version: "v2_2_000"
    base_url: "{{ grouper.ws.base_url }}"
  max_heap: "{{ max_heap.low }}"

# Optional role to stand up a basic iRODS iCat.
irods:
  admin: rodsadmin
  admins: CHANGEME # we don't know what all this means, let's ask dennis
  admin_users: CHANGEME
  admin_password: CHANGEME
  bad_chars: \u0060\u0027\u000A\u0009
  # used by staff to create permanent id requests
  data_curators_group: data_curators
  default_resource: "demoResc"
  default_resource_path: "/var/lib/irods/iRODS/demoResc"
  federation_host: 
  home: CHANGEME # /zone/home format do not put a trailing slash on there or you will get path not found
  host: CHANGEME
  icat:
    host: CHANGEME
    db: CHANGEME
    dbport: 5432
    password: CHANGEME
    user: CHANGEME
  password: CHANGEME
  port: 1247
  portrange:
    start: 20000
    end: 20199
  resc: demoResc
  service_acct: irods
  service_grp: irods
  # irods.user is used to authenticate DE to irods
  user: CHANGEME
  vault: CHANGEME
  zone: CHANGEME

# Jex does not actually have a container. The container name is how most syslog entries
# are identified. See rsyslog-config role.

jex_host:  "{{ groups['jex'][0] }}"
jex_port: 5004
jex:
 host: "{{ groups['jex'][0] }}"
 port: "{{jex_port}}"
 base: "http://{{ jex_host }}:{{jex_port}}"
 service_name: jex.service
 service_name_short: jex
 compose_service: jex
 log_file: jex/jex.log
 container_name: jex
 batch_group: batch_processing
 nfs_base: /export/condor
 icommands_path: /usr/local/icommands/:/usr/local/bin/:/usr/bin/
 request_disk: 0

# Jex database settings
jexdb:
  driver: "{{db_driver}}"
  host: "{{db_host}}"
  db: jex
  password: CHANGEME
  port: "{{db_port}}"
  targz: https://everdene.iplantcollaborative.org/jenkins/job/databases-dev/lastSuccessfulBuild/artifact/databases/jex-db/jex-db.tar.gz
  user: CHANGEME
  vendor: "{{db_vendor}}"

# jexevents is an intermediary service between Jex and Condor Log Monitor.
jexevents_host: "{{ groups['jexevents'][0] }}"
jexevents_port: 5005
jexevents:
 host: "{{jexevents_host}}"
 port: "{{jexevents_port}}"
 base: "http://{{ jexevents_host }}:{{jexevents_port}}/"
 event_url: "{{ apps.base }}/callbacks/de-job"
 service_name: jexevents.service
 service_name_short: jex-events
 compose_service: jex_events
 service_description: jex events service
 image_name: jex-events
 container_name: jex-events
 properties_file: jexevents.properties
 log_file: jex-events-docker.log

job_status_poll_interval: 15

# DE uses JSON Web Tokens for persistent internal authentication.
jwt:
  signing_key:
    private: "{{ global_config_dir }}/crypto/private-key.pem"
    public: "{{ global_config_dir }}/crypto/public-key.pem"
    password: CHANGEME
    algorithm: "rs256"
  accepted_keys:
    dir: "{{ global_config_dir }}/crypto/accepted_keys"
  validity_window:
    end: 300
  wso2:
    header: x-jwt-assertion-iplant-org

kifshare_external_url: "http://{{ de.host }}/{{ kifshare.external_url_suffix }}"
# Kifshare allows external anonymous access to iRODS via URL.
kifshare:
  host: "{{ groups['kifshare'][0] }}"
  port: 1025
  download_buffer_size: 100
  # hard-coding url suffix here so Ansible doesn't crab.
  external_url: "http://{{ de.host }}/dl"
  external_url_suffix: dl
  service_name: kifshare.service
  service_name_short: kifshare
  compose_service: kifshare
  service_description: kifshare service
  image_name: kifshare
  container_name: kifshare
  properties_file: kifshare.properties
  log_file: kifshare-docker.log
  de_url: \{\{url\}\}/d/\{\{ticket-id\}\}/\{\{filename\}\}
  mode: prod
  max_heap: "{{ max_heap.low }}"

# Call the CAS/LDAP playbook if not using your organization's existing services.
# Test users may be specified using the LDIF template in the LDAP role.
ldap_done_dir: /etc/ansible/.done
ldap:
  conf_dir: /etc/openldap
  host: "{{ groups['ldap'][0] }}"
  dc: dc=your,dc=org,dc=edu
  domain_name: your.org.edu
  country: CHANGEME
  state: CHANGEME
  location: CHANGEME
  organization: CHANGEME
  # this is the password for the ldap root user
  admin_password: CHANGEME
  authn_format:
  enable_ssl: false
  base_dn: CHANGEME
  create_user_and_groups_done: "{{ldap_done_dir}}/create-users-and-groups"
  create_indexes_done: "{{ldap_done_dir}}/create-indexes"
  create_autofs_done: "{{ldap_done_dir}}/create-autofs"
  create_automount_done: "{{ldap_done_dir}}/create-automount"
  create_sudo_done: "{{ldap_done_dir}}/create-sudo"
  create_sudo_master_done: "{{ldap_done_dir}}/create-sudo-master"
  create_testde_done: "{{ldap_done_dir}}/create-testde"
  dpkg_reconfigure: false
  genadminpw: CHANGE
  global_user: root
  global_use_sudo: false
  group_base_dn: ou=Groups,dc=your,dc=org,dc=edu
  include_create_user_and_groups: true
  include_create_indexes: true
  include_create_autofs: false
  include_create_automount: false
  include_create_sudo: false
  include_create_sudo_master: false
  include_testde: true
  manager_dn: cn=Manager,dc=your,dc=org,dc=edu
  port: 389
  # this is the password for the ldap manager
  manager_password: CHANGEME
  password_size: 8
  search_base_dn: ou=Users,dc=your,dc=org,dc=edu
  slapd_dpkg_reconfigure_done: "{{ldap.done_dir}}/slapd-dpkg-reconfigure-done"
  tld: CHANGEME
  trusted_cert:
  use_starttls: false
  user: ldap
  g_suffix: Groups
  u_suffix: Users
  m_suffix: Machines

logging:
  dir: /var/log/de
  conf_dir: "{{de_config_dir}}/logging"

# called in logstash.yaml and support_logstash_config
logstash_elasticsearch_host: "{{ groups['de-elk'][0] }}"
logstash:
  port: 5000
  ssl:
    key: CHANGEME
    cert: CHANGEME

logstash_forwarder:
  service_description: logstash forwarder service
  service_name: logstash-forwarder.service
  service_name_short: logstash-forwarder
  image_name: willdurand/logstash-forwarder
  container_name: logstash-fowarder

max_edit_file_size: 2147483647

# The REST API for the Discovery Environment Metadata services.
metadata_db_driver: "{{ db_driver }}"
metadata_db_vendor: "{{ db_vendor }}"
metadata_db_host: "{{ db_host }}"
metadata_db_port: "{{ db_port }}"
metadata_db_user: CHANGEME
metadata_db_password: CHANGEME
metadata_db_name: metadata
metadata_db_admin: "{{ db_admin }}"
metadata_db_admin_password: "{{ db_admin_password }}"
metadata_db_targz: https://everdene.iplantcollaborative.org/jenkins/job/databases-dev/lastSuccessfulBuild/artifact/databases/metadata/metadata-db.tar.gz

# REST API for the Discovery Environment Metadata services.
metadata:
  host: "{{ groups['metadata'][0] }}"
  port: 5013
  service_name: metadata.service
  service_name_short: metadata
  compose_service: metadata
  service_description: metadata service
  image_name: metadata
  container_name: metadata
  properties_file: metadata.properties
  log_file: metadata-docker.log
  max_heap: "{{ max_heap.high }}"


# Monkey synchronizes the tag documents in the data search index with the metadata database.
monkey:
  host: "{{ groups['monkey'][0] }}"
  service_name: monkey.service
  service_name_short: monkey
  compose_service: monkey
  service_description: monkey service
  image_name: monkey
  container_name: monkey
  properties_file: monkey.properties
  log_file: monkey-docker.log
  max_heap: "{{ max_heap.low }}"

# iptables ranges for specific network needs. modify templates as you see fit.
net:
  campus: CHANGEME
  dmz: CHANGEME
  trust: CHANGEME
  vpn: CHANGEME
  wifi: CHANGEME
  docker: CHANGEME # this is the ip of the docker bridge, typically 192.0.0.0/8 to allow containers to talk to each other

# Nginx handles SSL and proxy/auth for the UI container.
nginx_ssl:
  server_name: "{{ groups['ui'][0] }}"
  cert: /etc/iplant/ssl/iplant.crt
  cert_key: /etc/iplant/ssl/iplant.key

notificationagent_base: "http://{{ notificationagent_host }}:{{ notificationagent.port }}:5008"
notificationagent_host: "{{ services_host }}"

notificationagent:
  host: "{{ groups['notificationagent'][0] }}"
  port: 5008
  base: "http://{{ groups['notificationagent'][0] }}:5008"
  service_name: notification-agent.service
  service_name_short: notificationagent
  compose_service: notification_agent
  service_description: notification agent service
  image_name: notification-agent
  container_name: notificationagent
  properties_file: notificationagent.properties
  log_file: notificationagent-docker.log
  clean_start: "1:45:00"
  clean_age: 90
  clean_enable: "true"
  max_heap: "{{ max_heap.low }}"

notification_db_driver: "{{ db_driver }}"
notification_db_vendor: "{{ db_vendor }}"
notification_db_port: "{{ db_port }}"
notification_db_host: "{{ db_host }}"
notification_db_name: notifications
notification_db_password: CHANGEME
notification_db_user: CHANGEME
notification_db_admin: "{{ db_admin }}"
notification_db_admin_password: "{{ db_admin_password }}"
notification_db_targz: https://everdene.iplantcollaborative.org/jenkins/job/databases-dev/lastSuccessfulBuild/artifact/databases/notification-db/notification-db.tar.gz

path_list_file_identifier: "# application/vnd.de.path-list+csv; version=1"
path_list_info_type: ht-analysis-path-list

permanent_id:
  admin_email_addr: "{{ de_mail_to_addr }}"
  email_from_addr: "{{ de_mail_from_addr }}"
  ezid:
    username: apitest
    password: CHANGEME
    shoulders:
      ark: "ark:/99999/fk4"
      doi: "doi:10.5072/FK2"

pgp_keyring_path: "{{ gpg_home_dir }}/secring.gpg"
pgp_key_password: CHANGEME

prod_deployment: false

# proxy_env added for RENCI whose gateway requires it.
# This setting is silently ignored if left unset.
proxy_env:
  http_maven_proxy_host:
  http_maven_proxy_port:
  https_proxy: 
  http_proxy:
  use_proxy: false
  proxy_user:
  proxy_password:
  proxy_port: 8080
  non_proxy_hosts: # e.g. .example.org,localhost,127.0.0.1

# optional R Server role for use with Dataverse
rserve:
  host: "{{ groups['rserve'][0] }}"
  user: rserve
  pass: CHANGEME
  port: 6311

saved_searches_log_file: /home/iplant/logs/saved-searches.log
saved_searches:
  host: "{{ groups['saved-searches'][0] }}"
  port: 5009
  service_name: saved-searches.service
  service_name_short: saved-searches
  compose_service: saved_searches
  service_description: saved searches services
  image_name: saved-searches
  container_name: saved-searches
  properties_file: saved-searches.properties
  log_file: saved-searches-docker.log
  max_heap: "{{ max_heap.low }}"

search_default_limit: 200

# new to dev branch, probably don't need yet
templeton_incremental:
 host: "{{ groups['templeton-incremental'][0] }}"
 port:
 service_name_short: templeton-incremental
 compose_service: templeton_incremental
 service_description: templeton incremental service
 image_name: templeton
 container_name: templeton-incremental
 log_file: templeton-docker.log

# Templeton indexes the metadata Postgres database.
templeton_periodic:
 host: "{{ groups['templeton-periodic'][0] }}"
 port:
 service_name_short: templeton-periodic
 compose_service: templeton_periodic
 service_description: templeton periodic service
 image_name: templeton
 container_name: templeton-periodic
 log_file: templeton-docker.log

# Terrain provides the primary REST API used by the Discovery Environment. 
# Its role is to validate user authentication and to coordinate calls to other web services.
terrain:
  host: "{{ groups['terrain'][0] }}"
  port: 5007
  base: "http://{{ groups['terrain'][0] }}:5007"
  service_name: terrain.service
  service_name_short: terrain
  compose_service: terrain
  service_description: terrain service
  image_name: terrain
  container_name: terrain
  properties_file: terrain.properties
  log_file: terrain-docker.log
  max_heap: "{{ max_heap.high }}"

tree_parser_base: http://portnoy.iplantcollaborative.org/parseTree

# tree_urls manages saved searches in DE.
tree_urls:
  host: "{{ groups['tree-urls'][0] }}"
  port: 5010
  service_name: tree-urls.service
  service_name_short: tree-urls
  compose_service: tree_urls
  service_description: Tree urls service
  image_name: tree-urls
  container_name: tree-urls
  properties_file: tree-urls.properties
  log_file: tree-urls-docker.log
  cleanup_age: 30
  cleanup_start: "1:30:00"
  cleanup_enable: "true"
  avu: tree-urls
  max_heap: "{{ max_heap.low }}"


#user_preferences_host: "{{ services_host }}"
user_preferences_log_file: /home/iplant/logs/user-preferences.log
user_preferences:
  host: "{{ groups['user-preferences'][0] }}"
  port: 5011
  service_name: user-preferences.service
  service_name_short: user-preferences
  compose_service: user_preferences
  service_description: user preferences service
  image_name: user-preferences
  container_name: user-preferences
  properties_file: user-preferences.properties
  log_file: user-preferences-docker.log
  max_heap: "{{ max_heap.low }}"

user_sessions_host: "{{ services_host }}"
user_sessions_log_file: /logs/user-sessions.log
user_sessions:
  host: "{{ groups['user-sessions'][0] }}"
  port: 1834
  service_name: user-sessions.service
  service_name_short: user-sessions
  compose_service: user_sessions
  service_description: user sessions service
  image_name: user-sessions
  container_name: user-sessions
  properties_file: user-sessions.properties
  log_file: user-sessions-docker.log
  max_heap: "{{ max_heap.low }}"


time: "{{ansible_date_time.date}}:{{ansible_date_time.time}}"

# wants America/New_York, for example.
timezone: CHANGEME
